# -*- coding: utf-8 -*-
"""text_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14BAX9rwTvKd9XZ7NS8MkF7pJddJ8K7kh

# Importing all the dependency
"""

import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

!pip install transformers[sentencepiece] -q

!pip install transformers datasets evaluate

!pip install tensorflow-text

import tensorflow as tf
import numpy as np
import pandas as pd
import re
import torch
import os
from transformers import AutoTokenizer, DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, logging
from datasets import Dataset
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import precision_recall_fscore_support as score
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder

!pip install tensorflow-addons

!pip install tf-models-official

import numpy as np
import pandas as pd

from sklearn import preprocessing
from sklearn.model_selection import train_test_split

import tensorflow as tf
import tensorflow_addons as tfa
import tensorflow_hub as hub
import tensorflow_text as text  
from keras.utils import to_categorical
from official.nlp import optimization  
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix

# Plots
import matplotlib.pyplot as plt
import seaborn as sns

from keras.utils import to_categorical

import tensorflow_hub as hub

"""Loading the train and test data set"""

df_train = pd.read_csv("/content/train.csv")
df_test = pd.read_csv("/content/test.csv")

print(df_train)
print(df_test)

df_train.head()

df_train['Text']= df_train['Title']+ ' ' + df_train['Description']

df_test['Text']= df_test['Title']+ ' ' + df_test['Description']

df_train.head()

df_train = df_train.drop(['Title', 'Description'], axis=1)
df_test = df_test.drop(['Title', 'Description'], axis=1)

df_train.head()

df_train = df_train.rename(columns={'Class Index': 'Label'})

df_test = df_test.rename(columns={'Class Index': 'Label'})

TEXT_LABELS = {1: "World", 2: "Sports", 3: "Business", 4: "Sci/Tech"}

print('Shape of the training data: ', df_train.shape)
print('Shape of the test data: ', df_test.shape)

"""The train set had 120000 text i.e, 30000 text in each label"""

df_train.groupby('Label').count()



max_rows_per_label_train = 2000
train = df_train.groupby('Label').apply(lambda x: x.sample(min(len(x), max_rows_per_label_train)))

max_rows_per_label_test = 500
test = df_test.groupby('Label').apply(lambda x: x.sample(min(len(x), max_rows_per_label_test)))

train.head()

train = train.reset_index(drop=True)
test = test.reset_index(drop=True)

"""From 30000 text im keeping 2000 text for each label to easy my training process"""

train.groupby('Label').count()

"""Also there are 500 text each for testing"""

test.groupby('Label').count()

train.head()

"""# Pre-Processing """

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word.lower() not in stopwords.words('english')]
    return " ".join(words)

train['Category'] = train['Label'].map(TEXT_LABELS)

train.groupby('Category').Label.value_counts().plot(kind = "bar", color = ["pink", "orange", "red", "yellow", "blue"])
plt.xlabel("Category of data")
plt.title("Visulaize numbers of Category of data")
plt.show()

train['clean_text'] = train['Text'].apply(remove_stopwords)

test['clean_text']=test['Text'].apply(remove_stopwords)

def special_char(text):
  reviews = ''
  for x in text:
    if x.isalnum():
      reviews = reviews + x
    else:
      reviews = reviews + ' '
  return reviews

test.head()

test['clean_text']=test['clean_text'].apply(special_char)

def convert_lower(text):
   return text.lower()

test['clean_text']=test['clean_text'].apply(convert_lower)

test.head()

train['clean_text']=train['clean_text'].apply(special_char)

train['clean_text']=train['clean_text'].apply(convert_lower)

train['clean_text'][1]

train.sample()

le = LabelEncoder()

X_train = train['clean_text']
y_train = train['Label']
y_train = le.fit_transform(y_train)
y_train = to_categorical(y_train, num_classes=4)


X_test = test['clean_text']
y_test = test['Label']
y_test = le.transform(y_test)
y_test = to_categorical(y_test, num_classes=4)

print(X_test.shape)

print(y_test.shape)

train.shape[0]

"""# Loading the PreTrained BERT Model"""

text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
preprocessor = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
encoder_inputs = preprocessor(text_input)
encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4",
    trainable=True)
outputs = encoder(encoder_inputs)
pooled_output = outputs["pooled_output"]      
sequence_output = outputs["sequence_output"]

# Verify that the preprocessor works:
hub_inputs = preprocessor(['ID for each word, with zero padding at the end.'])
{key: value[0, :25].numpy() for key, value in hub_inputs.items()}

result = encoder(
    inputs=hub_inputs,
    training=False,
)

print("Pooled output shape:", result['pooled_output'].shape)
print("Sequence output shape:", result['sequence_output'].shape)

"""# Building the Model"""

epochs = 4
batch_size = 32
eval_batch_size = 32

train_data_size = train.shape[0]
steps_per_epoch = int(train_data_size / batch_size)
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 3e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

def build_model(num_classes, optimizer, max_len=512):
    
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3", name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4", trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(64, activation='relu')(net)
    net = tf.keras.layers.Dropout(0.1)(net)
    out = tf.keras.layers.Dense(num_classes, activation='softmax', name='classifier')(net)
    
    
    model = tf.keras.models.Model(text_input, out)
    model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model

model = build_model(num_classes=4, optimizer=optimizer)

model.summary()

checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

history = model.fit(x=X_train,
          y=y_train,
          validation_data=(X_test, y_test),
          epochs=epochs, 
          callbacks=[checkpoint, early_stopping], 
          batch_size=batch_size,
          validation_batch_size=eval_batch_size,
          verbose=1)

y_proba = model.predict(X_test, batch_size=eval_batch_size)

print(len(y_proba))
print(y_proba)

y_pred = np.argmax(y_proba, axis=1)

print(len(y_pred))
print(y_pred)

y_pred_labels = [TEXT_LABELS[x] for x in le.inverse_transform(y_pred)]
y_true_labels = [TEXT_LABELS[x] for x in test['Label']]

len(y_true_labels)

"""# Accuracy Check"""

print(classification_report(y_true=y_true_labels, y_pred=y_pred_labels))
print(confusion_matrix(y_true_labels, y_pred_labels))

print(y_true_labels)

"""# Check input Manually"""

text = ["Reuters - A car bomb exploded near a U.S.\military patrol in the town of Baiji, north of Baghdad, on\Tuesday, killing at least seven Iraqis and wounding 20 people,\including two U.S. soldiers, doctors and the military said."]

y_proba = model.predict(text)
y_pred = np.argmax(y_proba, axis=1)
y_pred_labels = [TEXT_LABELS[x] for x in le.inverse_transform(y_pred)]
print("\n\n",y_pred_labels[0])



